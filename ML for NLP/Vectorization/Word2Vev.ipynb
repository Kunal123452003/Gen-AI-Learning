{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bacbeb86",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8236f4",
   "metadata": {},
   "source": [
    "Word2Vec is a technique for NLP published in 2013.\n",
    "The word2vec algo uses a neural network model to learn word associations from a large corpus of text.\n",
    "Once trained, such a model can detect synonymous words or suggest additional words for a partial sentence.\n",
    "As the name implies, word2vec represents each distinct word with a paticular list/array of numbers called a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e57b14e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIn this the vector of a word is made using some features.\\nBelow is the example of how the vectors are made of each word:\\n\\nWORDS --->              Boy     Girl        King        Queen       Apple\\n     ^       gender      -1       1          -0.92       0.96        0.01   \\n     |       royal       0.01     0.03        0.98       0.99        -0.05\\nFeatures     age         0.03     0.09        0.87       0.86        0.01\\n     |       food        0.001   0.001       0.007       0.005       0.98\\n     |       .            .         .           .           .          .\\n     |       .            .         .           .           .          .\\n     |       .            .         .           .           .          .\\n     |      300th dim    \\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "In this the vector of a word is made using some features relation with that word .\n",
    "Below is the example of how the vectors are made of each word:\n",
    "\n",
    "WORDS ---> |            | Boy    | Girl       | King       | Queen     |  Apple\n",
    "------------------------|--------|------------|------------|-----------|-----------------\n",
    "           |  gender    |  -1    |   1        |  -0.92     |  0.96     |   0.01   \n",
    "     |     |  royal     |  0.01  |   0.03     |   0.98     |  0.99     |   -0.05\n",
    "Features   |  age       |  0.03  |   0.09     |   0.87     |  0.86     |   0.01\n",
    "     |     |  food      |  0.001 |  0.001     |  0.007     |  0.005    |   0.98\n",
    "     |     |  .         |   .    |     .      |     .      |     .     |     .\n",
    "     |     |  .         |   .    |     .      |     .      |     .     |     .\n",
    "     |     |  .         |   .    |     .      |     .      |     .     |     .\n",
    "     |     | nth feature|  0.01  |   0.02     |   0.03     |  0.04     |   0.05\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Through this we can see that the vector of each word is made using some features.\n",
    "like we get the vector of boy as:\n",
    "\n",
    "[ -1, 0.01, 0.03, 0.001, ..., 0.01]\n",
    "\n",
    "-1 means that the word boy related to the featuer gender is negative, \n",
    "0.01 means that the word boy related to the feature royal is positive and is very less related to it, \n",
    "0.03 means that the word boy related to the feature age is positive and is very less related to it,\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c2597fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFor finding the simialrity between two words we can use the cosine similarity formula:\\ndistance = 1 - consine_similarity(vector1, vector2)\\n\\nand the cosine similarity formula is given by:\\ncosine_similarity(a, b) = (a . b) / (||a|| * ||b||)\\n\\nwhere:\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "For finding the simialrity between two words we can use the cosine similarity formula:\n",
    "distance = 1 - consine_similarity(vector1, vector2)\n",
    "\n",
    "and the cosine similarity formula is given by:\n",
    "cosine_similarity(a, b) = (a . b) / (||a|| * ||b||)\n",
    "\n",
    "where:\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3834ef",
   "metadata": {},
   "source": [
    "#### We can tarin our own model or use any pre-trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce84a1a",
   "metadata": {},
   "source": [
    "## Word2Vec is of two types: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5b8f4cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1) CBOW (Continuous Bag of Words)\\n2) skipgram (Skip-gram)\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "1) CBOW (Continuous Bag of Words)\n",
    "2) skipgram (Skip-gram)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87803624",
   "metadata": {},
   "source": [
    "### Working\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6401a484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCBOW \\n\\nExample sentence = \"How are you i hope your life is good\"\\n\\nSteps:\\n1) we select a window size lets say 5\\n2) now we select the 1st 5 words of a sentence then we select the middle word as Output and rest will be input\\n    so for above ex , it wil be like  INPUT(How, are, i, hope) and OUTPUT(you)\\n3) now we will select 2nd 5 words and we will get INPUT(are, you, hope, your)  and OUTPUT(i)\\n4) The final data will be,\\n          INPUT                OUPUT\\n    how, are, i, hope           you\\n    are, you, hope, your         i\\n    you, i, your, life          hope\\n    i, hope, life, is           your\\n    hope, your, is good         life\\n\\n5) Make the OHE for the above input data,  so for \"how\" it can be (1,0,0,0,0,0,0,0,0) for \"are\" (0,1,0,0,0,0,0,0,0) \\n6) now we will train a neural network on the above data\\n7) the structure of the NN is \\n    input layer :  it will be the OHE of each word of a row\\n    middle layer/ hidden layer:     the no of neurons will be same as the size of the window (so here it is 5)\\n    ouput layer:        the no of neurons will we the size of vocabulary (unique words)\\n\\nthrough this we will get the vector of a word , ex : for 1st row   we will get the vector of \"you\" as [0.23,0.81,0.99,0.69,0.88]  as these values are the relation of \"you\" word with the \"how\",\"are\",\"you\",\"i\",\"hope\"\\n\\nwe can say that these values are the features used to make the vector of a word.\\n\\nin above word2vec section , i have made a table of words and 300 features ,  similarly we can change the window size to 300 and through this we will get the vectors(with 300 features relation) of each word  i.e. relation of a word with 300 other words   represented in a number and stored in a vector\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "CBOW \n",
    "\n",
    "Example sentence = \"How are you i hope your life is good\"\n",
    "\n",
    "Steps:\n",
    "1) we select a window size lets say 5\n",
    "2) now we select the 1st 5 words of a sentence then we select the middle word as Output and rest will be input\n",
    "    so for above ex , it wil be like  INPUT(How, are, i, hope) and OUTPUT(you)\n",
    "3) now we will select 2nd 5 words and we will get INPUT(are, you, hope, your)  and OUTPUT(i)\n",
    "4) The final data will be,\n",
    "          INPUT                OUPUT\n",
    "    how, are, i, hope           you\n",
    "    are, you, hope, your         i\n",
    "    you, i, your, life          hope\n",
    "    i, hope, life, is           your\n",
    "    hope, your, is good         life\n",
    "\n",
    "5) Make the OHE for the above input data,  so for \"how\" it can be (1,0,0,0,0,0,0,0,0) for \"are\" (0,1,0,0,0,0,0,0,0) \n",
    "6) now we will train a neural network on the above data\n",
    "7) the structure of the NN is \n",
    "    input layer :  it will be the OHE of each word of a row\n",
    "    middle layer/ hidden layer:     the no of neurons will be same as the size of the window (so here it is 5)\n",
    "    ouput layer:        the no of neurons will we the size of vocabulary (unique words)\n",
    "\n",
    "through this we will get the vector of a word , ex : for 1st row   we will get the vector of \"you\" as [0.23,0.81,0.99,0.69,0.88]  as these values are the relation of \"you\" word with the \"how\",\"are\",\"you\",\"i\",\"hope\"\n",
    "\n",
    "we can say that these values are the features used to make the vector of a word.\n",
    "\n",
    "in above word2vec section , i have made a table of words and 300 features ,  similarly we can change the window size to 300 and through this we will get the vectors(with 300 features relation) of each word  i.e. relation of a word with 300 other words   represented in a number and stored in a vector\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcc9b2c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSkipgram\\n\\nIn skipgram we do the opposite of CBOW, i.e. we take a word and find the context words around it.\\nHere output becomes input and input becomes output.\\n\\nHere is how it works:\\n1) We take the same example sentence: \"How are you i hope your life is good\"\\n2) We select a window size, let\\'s say 5.\\n3) Now we take the first word \"How\" and find the context words around it.\\n4) The context words for \"How\" will be the words within the window size of 5, which are \"are\", \"you\", \"i\", \"hope\".\\n5) So for the word \"How\", we will have INPUT(\"How\") and OUTPUT(\"are\", \"you\", \"i\", \"hope\").\\n6) Now we will repeat this for all the words in the sentence.\\n7) The final data will be:\\n          INPUT                OUTPUT\\n           How             are, you, i, hope\\n\\nAnd we again train the NN to get the vector of each word.\\n\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Skipgram\n",
    "\n",
    "In skipgram we do the opposite of CBOW, i.e. we take a word and find the context words around it.\n",
    "Here output becomes input and input becomes output.\n",
    "\n",
    "Here is how it works:\n",
    "1) We take the same example sentence: \"How are you i hope your life is good\"\n",
    "2) We select a window size, let's say 5.\n",
    "3) Now we take the first word \"How\" and find the context words around it.\n",
    "4) The context words for \"How\" will be the words within the window size of 5, which are \"are\", \"you\", \"i\", \"hope\".\n",
    "5) So for the word \"How\", we will have INPUT(\"How\") and OUTPUT(\"are\", \"you\", \"i\", \"hope\").\n",
    "6) Now we will repeat this for all the words in the sentence.\n",
    "7) The final data will be:\n",
    "          INPUT                OUTPUT\n",
    "           How             are, you, i, hope\n",
    "\n",
    "And we again train the NN to get the vector of each word.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b344af74",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "When to use skipgram and CBOW:\n",
    "\n",
    "When data is small we use CBOW as it is faster and gives good results.\n",
    "When data is large we use skipgram as it captures more context and gives better results.\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
